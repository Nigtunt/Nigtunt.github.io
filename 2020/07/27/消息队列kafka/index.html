<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><title>消息队列kafka | Nigtunt</title><meta name="description" content="消息队列kafka"><meta name="keywords" content="kafka"><meta name="author" content="Nigtunt"><meta name="copyright" content="Nigtunt"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.ico"><link rel="preconnect" href="//cdn.jsdelivr.net"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:title" content="消息队列kafka"><meta name="twitter:description" content="消息队列kafka"><meta name="twitter:image" content="https://ss0.bdstatic.com/70cFvHSh_Q1YnxGkpoWK1HF6hhy/it/u=892442603,2549350327&amp;fm=26&amp;gp=0.jpg"><meta property="og:type" content="article"><meta property="og:title" content="消息队列kafka"><meta property="og:url" content="https://nigtunt.github.io/2020/07/27/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97kafka/"><meta property="og:site_name" content="Nigtunt"><meta property="og:description" content="消息队列kafka"><meta property="og:image" content="https://ss0.bdstatic.com/70cFvHSh_Q1YnxGkpoWK1HF6hhy/it/u=892442603,2549350327&amp;fm=26&amp;gp=0.jpg"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://cdn.jsdelivr.net/npm/js-cookie/dist/js.cookie.min.js"></script><script>const autoChangeMode = 'false'
var t = Cookies.get("theme");
if (autoChangeMode == '1'){
const isDarkMode = window.matchMedia("(prefers-color-scheme: dark)").matches
const isLightMode = window.matchMedia("(prefers-color-scheme: light)").matches
const isNotSpecified = window.matchMedia("(prefers-color-scheme: no-preference)").matches
const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

if (t === undefined){
  if (isLightMode) activateLightMode()
  else if (isDarkMode) activateDarkMode()
  else if (isNotSpecified || hasNoSupport){
    console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
    now = new Date();
    hour = now.getHours();
    isNight = hour < 6 || hour >= 18
    isNight ? activateDarkMode() : activateLightMode()
}
} else if (t == 'light') activateLightMode()
else activateDarkMode()


} else if (autoChangeMode == '2'){
  now = new Date();
  hour = now.getHours();
  isNight = hour < 6 || hour >= 18
  if(t === undefined) isNight? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode() 
} else {
  if ( t == 'dark' ) activateDarkMode()
  else if ( t == 'light') activateLightMode()
}

function activateDarkMode(){
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null){
    document.querySelector('meta[name="theme-color"]').setAttribute('content','#000')
  }
}
function activateLightMode(){
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null){
  document.querySelector('meta[name="theme-color"]').setAttribute('content','#fff')
  }
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="canonical" href="https://nigtunt.github.io/2020/07/27/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97kafka/"><link rel="prev" title="docker compose and docker swarm" href="https://nigtunt.github.io/2020/07/28/docker-compose-and-docker-swarm/"><link rel="next" title="springcloud" href="https://nigtunt.github.io/2020/06/28/springcloud%E5%85%A5%E9%97%A8%E7%BA%A7%E7%AE%80%E4%BB%8B/"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容:${query}"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"cookieDomain":"https://xxx/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  highlight_copy: 'true',
  highlight_lang: 'true',
  highlight_shrink: 'false',
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    title: 'Snackbar.bookmark.title',
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  copyright: undefined,
  copy_copyright_js: false,
  ClickShowText: undefined,
  medium_zoom: 'false',
  Snackbar: undefined
  
}</script><meta name="generator" content="Hexo 4.2.1"></head><body><canvas class="fireworks"></canvas><div id="header"> <div id="page-header"><span class="pull_left" id="blog_name"><a class="blog_title" id="site-name" href="/">Nigtunt</a></span><i class="fa fa-bars fa-fw toggle-menu pull_right close" aria-hidden="true"></i><span class="pull_right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> 关于</span></a></div></div></span><span class="pull_right" id="search_button"><a class="site-page social-icon search"><i class="fa fa-search fa-fw"></i><span> 搜索</span></a></span></div></div><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="lazyload avatar_img" src="https://yhaq.top/group1/M00/00/00/head.png" onerror="onerror=null;src='/img/friend_404.gif'"></div><div class="mobile_post_data"><div class="mobile_data_item is_center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">132</div></a></div></div><div class="mobile_data_item is_center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">37</div></a></div></div><div class="mobile_data_item is_center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">15</div></a></div></div></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> 关于</span></a></div></div></div><div id="mobile-sidebar-toc"><div class="toc_mobile_headline">目录</div><ol class="toc_mobile_items"><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#kafka安装"><span class="toc_mobile_items-number">1.</span> <span class="toc_mobile_items-text">kafka安装</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#kafka基本的使用"><span class="toc_mobile_items-number">2.</span> <span class="toc_mobile_items-text">kafka基本的使用</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#kafka架构深入"><span class="toc_mobile_items-number">3.</span> <span class="toc_mobile_items-text">kafka架构深入</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#kafka工作流程以及文件存储机制"><span class="toc_mobile_items-number">3.1.</span> <span class="toc_mobile_items-text">kafka工作流程以及文件存储机制</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#kafka生产者"><span class="toc_mobile_items-number">3.2.</span> <span class="toc_mobile_items-text">kafka生产者</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#分区策略"><span class="toc_mobile_items-number">3.2.1.</span> <span class="toc_mobile_items-text">分区策略</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#数据可靠性保证"><span class="toc_mobile_items-number">3.2.2.</span> <span class="toc_mobile_items-text">数据可靠性保证</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#Exactly-Once语义"><span class="toc_mobile_items-number">3.2.3.</span> <span class="toc_mobile_items-text">Exactly Once语义</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#kafka消费者"><span class="toc_mobile_items-number">3.3.</span> <span class="toc_mobile_items-text">kafka消费者</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#消费方式"><span class="toc_mobile_items-number">3.3.1.</span> <span class="toc_mobile_items-text">消费方式</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#分区分配策略"><span class="toc_mobile_items-number">3.3.2.</span> <span class="toc_mobile_items-text">分区分配策略</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#offset的维护"><span class="toc_mobile_items-number">3.3.3.</span> <span class="toc_mobile_items-text">offset的维护</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#消费者组案例"><span class="toc_mobile_items-number">3.3.4.</span> <span class="toc_mobile_items-text">消费者组案例</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#kafka高效读写数据"><span class="toc_mobile_items-number">3.4.</span> <span class="toc_mobile_items-text">kafka高效读写数据</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#Zookeeper在kafka中的作用"><span class="toc_mobile_items-number">3.5.</span> <span class="toc_mobile_items-text">Zookeeper在kafka中的作用</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#kafka事务"><span class="toc_mobile_items-number">3.6.</span> <span class="toc_mobile_items-text">kafka事务</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#Producer事务"><span class="toc_mobile_items-number">3.6.1.</span> <span class="toc_mobile_items-text">Producer事务</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#Consumer事务"><span class="toc_mobile_items-number">3.6.2.</span> <span class="toc_mobile_items-text">Consumer事务</span></a></li></ol></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#kafka-API"><span class="toc_mobile_items-number">4.</span> <span class="toc_mobile_items-text">kafka API</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#Producer-API"><span class="toc_mobile_items-number">4.1.</span> <span class="toc_mobile_items-text">Producer API</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#消息发送流程"><span class="toc_mobile_items-number">4.1.1.</span> <span class="toc_mobile_items-text">消息发送流程</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#异步发送API"><span class="toc_mobile_items-number">4.1.2.</span> <span class="toc_mobile_items-text">异步发送API</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#同步发送"><span class="toc_mobile_items-number">4.1.3.</span> <span class="toc_mobile_items-text">同步发送</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#Consumer-API"><span class="toc_mobile_items-number">4.2.</span> <span class="toc_mobile_items-text">Consumer API</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#自动提交offest"><span class="toc_mobile_items-number">4.2.1.</span> <span class="toc_mobile_items-text">自动提交offest</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#手动提交offset"><span class="toc_mobile_items-number">4.2.2.</span> <span class="toc_mobile_items-text">手动提交offset</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#自定义存储offset"><span class="toc_mobile_items-number">4.2.3.</span> <span class="toc_mobile_items-text">自定义存储offset</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#自定义拦截器"><span class="toc_mobile_items-number">4.3.</span> <span class="toc_mobile_items-text">自定义拦截器</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#拦截器原理"><span class="toc_mobile_items-number">4.3.1.</span> <span class="toc_mobile_items-text">拦截器原理</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#拦截器案例"><span class="toc_mobile_items-number">4.3.2.</span> <span class="toc_mobile_items-text">拦截器案例</span></a></li></ol></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#kafka监控"><span class="toc_mobile_items-number">5.</span> <span class="toc_mobile_items-text">kafka监控</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#kafka-Eagle"><span class="toc_mobile_items-number">5.1.</span> <span class="toc_mobile_items-text">kafka Eagle</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#kafka面试题"><span class="toc_mobile_items-number">6.</span> <span class="toc_mobile_items-text">kafka面试题</span></a></li></ol></div></div><div id="body-wrap"><div id="web_bg" data-type="photo"></div><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true">     </i><div class="auto_open" id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#kafka安装"><span class="toc-number">1.</span> <span class="toc-text">kafka安装</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#kafka基本的使用"><span class="toc-number">2.</span> <span class="toc-text">kafka基本的使用</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#kafka架构深入"><span class="toc-number">3.</span> <span class="toc-text">kafka架构深入</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#kafka工作流程以及文件存储机制"><span class="toc-number">3.1.</span> <span class="toc-text">kafka工作流程以及文件存储机制</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#kafka生产者"><span class="toc-number">3.2.</span> <span class="toc-text">kafka生产者</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#分区策略"><span class="toc-number">3.2.1.</span> <span class="toc-text">分区策略</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#数据可靠性保证"><span class="toc-number">3.2.2.</span> <span class="toc-text">数据可靠性保证</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Exactly-Once语义"><span class="toc-number">3.2.3.</span> <span class="toc-text">Exactly Once语义</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#kafka消费者"><span class="toc-number">3.3.</span> <span class="toc-text">kafka消费者</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#消费方式"><span class="toc-number">3.3.1.</span> <span class="toc-text">消费方式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#分区分配策略"><span class="toc-number">3.3.2.</span> <span class="toc-text">分区分配策略</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#offset的维护"><span class="toc-number">3.3.3.</span> <span class="toc-text">offset的维护</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#消费者组案例"><span class="toc-number">3.3.4.</span> <span class="toc-text">消费者组案例</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#kafka高效读写数据"><span class="toc-number">3.4.</span> <span class="toc-text">kafka高效读写数据</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Zookeeper在kafka中的作用"><span class="toc-number">3.5.</span> <span class="toc-text">Zookeeper在kafka中的作用</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#kafka事务"><span class="toc-number">3.6.</span> <span class="toc-text">kafka事务</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Producer事务"><span class="toc-number">3.6.1.</span> <span class="toc-text">Producer事务</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Consumer事务"><span class="toc-number">3.6.2.</span> <span class="toc-text">Consumer事务</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#kafka-API"><span class="toc-number">4.</span> <span class="toc-text">kafka API</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Producer-API"><span class="toc-number">4.1.</span> <span class="toc-text">Producer API</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#消息发送流程"><span class="toc-number">4.1.1.</span> <span class="toc-text">消息发送流程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#异步发送API"><span class="toc-number">4.1.2.</span> <span class="toc-text">异步发送API</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#同步发送"><span class="toc-number">4.1.3.</span> <span class="toc-text">同步发送</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Consumer-API"><span class="toc-number">4.2.</span> <span class="toc-text">Consumer API</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#自动提交offest"><span class="toc-number">4.2.1.</span> <span class="toc-text">自动提交offest</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#手动提交offset"><span class="toc-number">4.2.2.</span> <span class="toc-text">手动提交offset</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#自定义存储offset"><span class="toc-number">4.2.3.</span> <span class="toc-text">自定义存储offset</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#自定义拦截器"><span class="toc-number">4.3.</span> <span class="toc-text">自定义拦截器</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#拦截器原理"><span class="toc-number">4.3.1.</span> <span class="toc-text">拦截器原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#拦截器案例"><span class="toc-number">4.3.2.</span> <span class="toc-text">拦截器案例</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#kafka监控"><span class="toc-number">5.</span> <span class="toc-text">kafka监控</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#kafka-Eagle"><span class="toc-number">5.1.</span> <span class="toc-text">kafka Eagle</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#kafka面试题"><span class="toc-number">6.</span> <span class="toc-text">kafka面试题</span></a></li></ol></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://ss0.bdstatic.com/70cFvHSh_Q1YnxGkpoWK1HF6hhy/it/u=892442603,2549350327&amp;fm=26&amp;gp=0.jpg)"><div id="post-info"><div id="post-title"><div class="posttitle">消息队列kafka</div></div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 发表于 2020-07-27<span class="post-meta__separator">|</span><i class="fa fa-history" aria-hidden="true"></i> 更新于 2021-06-10</time><span class="post-meta__separator mobile_hidden">|</span><span class="mobile_hidden"><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/">消息队列</a></span><div class="post-meta-wordcount"><i class="fa fa-file-word-o post-meta__icon" aria-hidden="true"></i><span>字数总计: </span><span class="word-count">7.1k</span><span class="post-meta__separator">|</span><i class="fa fa-clock-o post-meta__icon" aria-hidden="true"></i><span>阅读时长: 27 分钟</span><span class="post-meta__separator">|</span><i class="fa fa-eye post-meta__icon" aria-hidden="true">       </i><span>阅读量: </span><span id="busuanzi_value_page_pv"></span></div></div></div></div><div class="layout layout_post" id="content-inner">   <article id="post"><div class="article-container" id="post-content"><html><head></head><body><h1 id="kafka安装"><a href="#kafka安装" class="headerlink" title="kafka安装"></a>kafka安装</h1><p>首先去官网下载 <a href="http://kafka.apache.org/downloads" target="_blank" rel="noopener">http://kafka.apache.org/downloads</a> </p>
<p>下载完成解压</p>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@iz2zege5xssb2avm6if6l8z kafka]<span class="comment"># tar -xvf kafka_2.12-2.5.0.tgz </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改两个配置文件</span></span><br><span class="line">[root@iz2zege5xssb2avm6if6l8z config]<span class="comment"># vim zookeeper.properties </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># the directory where the snapshot is stored.</span></span><br><span class="line">dataDir=/tmp/kafka/zookeeper   <span class="comment"># 主要修改存储位置</span></span><br><span class="line"><span class="comment"># kafka自带zookeeper</span></span><br><span class="line"></span><br><span class="line">[root@iz2zege5xssb2avm6if6l8z config]<span class="comment"># vim server.properties </span></span><br><span class="line">broker.id=0  <span class="comment"># 全局唯一的brokerid，不能重复</span></span><br><span class="line">delete.topic.enable=<span class="literal">true</span> <span class="comment"># 可以删除topic</span></span><br><span class="line">log.dirs=/tmp/kafka/kafka_logs <span class="comment"># 存储数据，</span></span><br><span class="line"></span><br><span class="line">zookeeper.connect=localhost:2181    <span class="comment"># 配置连接zookeeper集群地址</span></span><br></pre></td></tr></tbody></table></figure>

<h1 id="kafka基本的使用"><a href="#kafka基本的使用" class="headerlink" title="kafka基本的使用"></a>kafka基本的使用</h1><p>开启zookeeper</p>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@iz2zege5xssb2avm6if6l8z bin]<span class="comment"># ./zookeeper-server-start.sh ../config/zookeeper.properties</span></span><br></pre></td></tr></tbody></table></figure>

<p>开启kafka</p>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@iz2zege5xssb2avm6if6l8z bin]<span class="comment"># ./kafka-server-start.sh ../config/server.properties </span></span><br><span class="line"><span class="comment"># 守护进程方式运行</span></span><br><span class="line">[root@iz2zege5xssb2avm6if6l8z bin]<span class="comment"># ./kafka-server-start.sh -daemon ../config/server.properties</span></span><br></pre></td></tr></tbody></table></figure>

<p>创建一个主题</p>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@iz2zege5xssb2avm6if6l8z bin]<span class="comment"># ./kafka-topics.sh --create --zookeeper localhost:2181 --topic first --partitions 3 --replication-factor 1</span></span><br><span class="line">Created topic first.</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看所有topic</span></span><br><span class="line">[root@iz2zege5xssb2avm6if6l8z bin]<span class="comment"># ./kafka-topics.sh --zookeeper localhost:2181 --list</span></span><br><span class="line">first</span><br><span class="line"></span><br><span class="line">--topic 定义topic名</span><br><span class="line">--replication-factor 定义副本</span><br><span class="line">--partitions 定义分区数</span><br></pre></td></tr></tbody></table></figure>

<p>删除主题</p>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@iz2zege5xssb2avm6if6l8z bin]<span class="comment"># ./kafka-topics.sh --delete --zookeeper localhost:2181 --delete --topic first</span></span><br></pre></td></tr></tbody></table></figure>

<p>使用bin下的kafka-console-producer.sh 发送消息</p>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@iz2zege5xssb2avm6if6l8z bin]<span class="comment"># ./kafka-console-producer.sh --broker-list localhost:9092 --topic first</span></span><br><span class="line">>tes^H^H^H^H</span><br><span class="line">>hello owor^H^H^Hkldwa</span><br><span class="line">>hello world</span><br><span class="line">><span class="built_in">exit</span></span><br><span class="line">>stop</span><br></pre></td></tr></tbody></table></figure>

<p>使用kafka-console-consumer接收消息</p>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@iz2zege5xssb2avm6if6l8z bin]<span class="comment"># ./kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic first</span></span><br><span class="line">hello world</span><br><span class="line"><span class="built_in">exit</span></span><br><span class="line"><span class="built_in">exit</span>()</span><br><span class="line">stop</span><br><span class="line"></span><br><span class="line">--from-beginning  会把主题所有数据都读出来</span><br></pre></td></tr></tbody></table></figure>

<p>查看某个topic的详情</p>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@iz2zege5xssb2avm6if6l8z bin]<span class="comment"># ./kafka-topics.sh --zookeeper localhost:2181 --describe --topic first</span></span><br><span class="line">Topic: first    PartitionCount: 3       ReplicationFactor: 1    Configs: </span><br><span class="line">        Topic: first    Partition: 0    Leader: 0       Replicas: 0     Isr: 0</span><br><span class="line">        Topic: first    Partition: 1    Leader: 0       Replicas: 0     Isr: 0</span><br><span class="line">        Topic: first    Partition: 2    Leader: 0       Replicas: 0     Isr: 0</span><br></pre></td></tr></tbody></table></figure>

<p><strong>修改分区数</strong>   只能增加不能减少</p>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@iz2zege5xssb2avm6if6l8z bin]<span class="comment"># ./kafka-topics.sh --zookeeper localhost:2181 --alter --topic first --partitions 6</span></span><br></pre></td></tr></tbody></table></figure>

<h1 id="kafka架构深入"><a href="#kafka架构深入" class="headerlink" title="kafka架构深入"></a>kafka架构深入</h1><h2 id="kafka工作流程以及文件存储机制"><a href="#kafka工作流程以及文件存储机制" class="headerlink" title="kafka工作流程以及文件存储机制"></a>kafka工作流程以及文件存储机制</h2><p><img alt="image-20200722214535589" data-src="http://yhaq.top/group1/M00/00/02/rBGWVV8b9TuAadiRAAMO69UjTfY899.png" class="lazyload"></p>
<p>kakfa中消息是以topic进行分类的，生产者生成消息，消费者消费消息，都是面向topic的</p>
<p>topic是逻辑上的概念，而partition是物理上的概念，每个partition对应于一个log文上件，该log文件中存储的就</p>
<p>是producer生产的数据。Producer生产的数据会被不断追加到该log文件末端，且每条数据都有自己的offset。 消</p>
<p>费者组中的每个消费者，都会实时记录自己消费到了哪个offset，以便出错恢复时，从上次的位置继续消费。</p>
<p><strong>文件存储机制</strong></p>
<p><img alt="image-20200723113927179" data-src="http://yhaq.top/group1/M00/00/02/rBGWVV8eW12ALGZDAAIanxtnDgw322.png" class="lazyload"></p>
<p>由于生产者生产的消息会不断追加到log文件末尾，为防止log文件过大导致数据定位效率低下，Kafka 采取了分片</p>
<p>和索引机制，将每个partition分为多个segment。每个segment对应两个文件一“index”文件和“.log”文件。这些</p>
<p>文件位于一个文件夹下，该文件夹的命名规则为: topic 名称+分区序号。例如，first 这个topic有三个分区，则其</p>
<p>对应的文件夹为first-0,first-1，first-2。</p>
<p><img alt="image-20200723114152270" data-src="http://yhaq.top/group1/M00/00/02/rBGWVV8eW12AWj4jAAGbFBG98jE305.png" class="lazyload"></p>
<p>index文件和log文件</p>
<p><img alt="image-20200723114619241" data-src="http://yhaq.top/group1/M00/00/02/rBGWVV8eW16AMfMSAALkEoC6rms397.png" class="lazyload"></p>
<p>首先通过offest找到对应的index文件，然后找到offset对应的地址756，然后756+偏移地址，找到对应的message</p>
<h2 id="kafka生产者"><a href="#kafka生产者" class="headerlink" title="kafka生产者"></a>kafka生产者</h2><h3 id="分区策略"><a href="#分区策略" class="headerlink" title="分区策略"></a>分区策略</h3><p><strong>1)分区的原因</strong><br>    (1)方便在集群中扩展,每个Partition可以通过调整以适应它所在的机器，而一个topic又可以有多个Partition组成，因此整个集群就可以适应任意大小的数据了。</p>
<p>​    (2)可以提高并发，因为可以以Partition为单位读写了。</p>
<p><strong>2)分区的原则</strong></p>
<p><img alt="image-20200723115234983" data-src="http://yhaq.top/group1/M00/00/02/rBGWVV8eW16Aev-aAAI0Pd0WkfU825.png" class="lazyload"></p>
<p>我们需要将producer发送的数据封装成一个ProducerRecord对象。</p>
<p>​    (1)指明partition 的情况下，直接将指明的值直接作为partiton 值。</p>
<p>​    (2)没有指明partition 值但有key 的情况下,将key的hash 值与topic 的partition数进行取余得到partition 值。</p>
<p>​    (3)既没有partition值又没有key 值的情况下，第一次调用时随机生成一个整数(后面每次调用在这个整数上自增),将这个值与topic可用的partition 总数取余得到partition值，也就是常说的round-robin 算法。</p>
<h3 id="数据可靠性保证"><a href="#数据可靠性保证" class="headerlink" title="数据可靠性保证"></a>数据可靠性保证</h3><p>​    为保证producer发送的数据，能可靠的发送到指定的topic, topic 的每个partition收到producer发送的数据后，都需要向producer发送ack ( acknowledgement确认收到)，如果producer收到ack，就会进行下一轮的发送，否则重新发送数据。</p>
<p><img alt="image-20200723115715847" data-src="http://yhaq.top/group1/M00/00/02/rBGWVV8eW16AWZS8AAMEfWa_dvI512.png" class="lazyload"></p>
<p><strong>1）副本同步策略</strong></p>
<table>
<thead>
<tr>
<th>方案</th>
<th>优点</th>
<th>缺点</th>
</tr>
</thead>
<tbody><tr>
<td>半数以上完成同步，就发送ack</td>
<td>延迟低</td>
<td>选举新的leader时，容忍n台节点的故障，需要2n+1个副本。</td>
</tr>
<tr>
<td>全部完成同步才发送ack</td>
<td>选举新的leader时，容忍n台节点的故障，需要n+1个副本。</td>
<td>延迟高</td>
</tr>
</tbody></table>
<p>Kafka选择了第二种方案，原因如下:<br>1.同样为了容忍n台节点的故障，第一 种方案需要2n+1个副本，而第二种方案只需要n+1个副本，而Kafka的每个分区都有大量的数据，第一种方案会造成大量数据的冗余</p>
<p>2.虽然第二种方案的网络延迟会比较高，但网络延迟对Kafka的影响较小</p>
<p><strong>2）ISR</strong></p>
<p>​    采用第二种方案之后，设想以下情景: leader收到数据,所有follower 都开始同步数据，但有一个follower,因为某种故障,迟迟不能与leader进行同步，那leader就要一直等下去，直到它完成同步，才能发送ack。这个问题怎么解决呢?。<br>​    Leader维护了一个动态的in-sync replica set (ISR)，意为和leader 保持同步的follower集合。当ISR中的follower完成数据的同步之后,leader就会给follower发送ack.如果follower长时间未向leader同步数据，则该follower将被踢出ISR，该时间阈值由replica.lag.time.max.ms参数设定。Leader 发生故障之后，就会从ISR中选举新的leader。</p>
<p><strong>3）ack应答机制</strong></p>
<p>​    对于某些不太重要的数据，对数据的可靠性要求不是很高，能够容忍数据的少量丢失，所以没必要等ISR中的follower全部接收成功。</p>
<p>​    所以Kafka为用户提供了三种可靠性级别，用户根据对可靠性和延迟的要求进行权衡，选择以下的配置。</p>
<p><strong>acks参数配置:</strong><br>    <strong>acks:</strong><br>    0: producer 不等待broker的ack,这一操作提供了一个最低的延迟，broker一接收到还没有写入磁盘就已经返回，当broker故障时有可能丢失数据;</p>
<p>​    1: producer 等待broker的ack, partition 的leader落盘成功后返回ack，如果在follower同步成功之前leader故障，那么将会丢失数据;</p>
<p><img alt="image-20200723135522071" data-src="http://yhaq.top/group1/M00/00/02/rBGWVV8eW16AQ0rTAAE7kd9s3sE099.png" class="lazyload"></p>
<p>​    -1(all) : producer 等待broker的ack, partition的leader和follower全部落盘成功后才返回ack。但是如果在follower同步完成后，broker 发送ack之前，leader 发生故障，那么会造成数据重复。</p>
<p><img alt="image-20200723135617874" data-src="http://yhaq.top/group1/M00/00/02/rBGWVV8eW16AULTYAAEP5rBZHZw264.png" class="lazyload"></p>
<p><strong>4）故障处理细节</strong></p>
<p>​    Log文件中的HW和LEO</p>
<p><img alt="image-20200723224827759" data-src="http://yhaq.top/group1/M00/00/02/rBGWVV8eW16AF0vCAAKBbHUAN74138.png" class="lazyload"></p>
<p>LEO:指的是每个副本最大的offset</p>
<p>HW:指的是消费者能见到的最大的offset, ISR队列中最小的LEO</p>
<p><strong>(1) follower 故障</strong><br>    follower发生故障后会被临时踢出ISR,待该follower恢复后，follower会读取本地磁盘记录的上次的HW，并将log文件高于HW的部分截取掉，从HW开始向leader 进行同步。等该follower的LEO大于等于该Partition的HW，即follower追上leader之后，就可以重新加入ISR了。</p>
<p><strong>(2) leader故障</strong><br>    leader发生故障之后，会从ISR中选出一个新的leader之后，为保证多个副本之间的数据一致性,其余的follower会先将各自的log文件高于HW的部分截掉,然后从新的leader同步数据。</p>
<h3 id="Exactly-Once语义"><a href="#Exactly-Once语义" class="headerlink" title="Exactly Once语义"></a>Exactly Once语义</h3><p>​    将服务器的ACK级别设置为-1,可以保证Problucer到Server之间不会丢失数据，即At Least Once语义。相对的，将服务器ACK级别设置为0，可以保证生产者每条消息只会被发送一次，即AtMostOnce语义。</p>
<p>​    At Least Once可以保证数据不丢失，但是不能保证数据不重复;相对的，AtLeast Once可以保证数据不重复，但是不能保证数据不丢失。但是，对于一些非常重要的信息，比如说交易数据，下游数据消费者要求数据既不重复也不丢失，即Exactly Once语义。在0.11版本以前的Kafka,对此是无能为力的，只能保证数据不丢失，再在下游消费者对数据做全局去重。对于多个下游应用的情况，每个都需要单独做全局去重,这就对性能造成了很大影响。</p>
<p>​    0.11版本的Kafka，引入了一项重大特性：幂等性。所谓的幂等性就是指Producer不论向Server 发送多少次重复数据，Server 端都只会持久化一条。幂等性结合At Least Once语义，就构成了Kafka的Exactly Once语义。即:</p>
<p>​                            At Least Once + 幂等性=Exactly Once</p>
<p>​    要启用幂等性，只需要将Producer的参数中enable.idompotence设置为true即可。Kafka的幂等性实现其实就是将原来下游需要做的去重放在了数据上游。开启幂等性的Producer 在初始化的时候会被分配一个PID，发往同一Partition 的消息会附带Sequence Number。而Broker端会对<PID, Partition, SeqNumber>做缓存，当具有相同主键的消息提交时，Broker 只会持久化一条。</p>
<p>​    但是PID重启就会变化，同时不同的Partition也具有不同主键，所以幂等性无法保证跨分区跨会话的Exactly Once。</p>
<h2 id="kafka消费者"><a href="#kafka消费者" class="headerlink" title="kafka消费者"></a>kafka消费者</h2><h3 id="消费方式"><a href="#消费方式" class="headerlink" title="消费方式"></a>消费方式</h3><p>​    consumer采用pull (拉)模式从broker中读取数据。</p>
<p>​    push(推)模式很难适应消费速率不同的消费者,因为消息发送速率是由broker决定的。它的目标是尽可能以最快速度传递消息，但是这样很容易造成consumer来不及处理消息，典型的表现就是拒绝服务以及网络拥塞。而pull 模式则可以根据consumer的消费能力以适当的速率消费消息。</p>
<p>​    pull模式不足之处是，如果kafka没有数据，消费者可能会陷入循环中，一直返回空数据。针对这一点，Kafka的消费者在消费数据时会传入一个时长参数timeout,如果当前没有数据可供消费，consumer 会等待一段时 间之后再返回，这段时长即为timeout。</p>
<h3 id="分区分配策略"><a href="#分区分配策略" class="headerlink" title="分区分配策略"></a>分区分配策略</h3><p>​    一个consumer group中有多个consumer, 一个topic 有多个partition, 所以必然会涉及到partition的分配问题，即确定那个partition由哪个consumer来消费。</p>
<p>​    Kafka有两种分配策略，一是RoundRobin，一是Range。</p>
<p><strong>1）RoundRobin</strong>   轮询 会将多个主题和为一个整体进行轮询</p>
<p><img alt="image-20200724143509979" data-src="http://yhaq.top/group1/M00/00/02/rBGWVV8eW16AfSCyAADrdvIyDy4808.png" class="lazyload"></p>
<p><strong>2）Range</strong>    按主题分为一个整体</p>
<h3 id="offset的维护"><a href="#offset的维护" class="headerlink" title="offset的维护"></a>offset的维护</h3><p>​    由于consumer在消费过程中可能会出现断电宕机等故障，consumer恢复后，需要从故障前的位置的继续消费，所以consumer需要实时记录自己消费到了哪个offset,以便故障恢复后继续消费。</p>
<p><img alt="image-20200724131821074" data-src="http://yhaq.top/group1/M00/00/02/rBGWVV8eW16AV7PzAAOkJMf_rUE936.png" class="lazyload"></p>
<p>​    Kafka0.9版本之前，consumer默认将offset 保存在Zookeeper中，从0.9版本开始，consumer默认将offset 保存在Kafka一个内置的topic中，该topic为__consumer_offsets。</p>
<p>1)修改配置文件consumer.properties</p>
<blockquote>
<p>exclude.internal.topics=false</p>
</blockquote>
<p>2)读取offset</p>
<p>0.11.0.0之前版本：</p>
<blockquote>
<p>bin/kafka-console-consumer.sh –topic __consumer_offsets –zookeeper localhost:2181 –formatter<br>”kafka. coordinator.GroupMetadataManager$offsetsMessageFormatter”<br>–consumer.config config/ consumer.properties –from-beginning</p>
</blockquote>
<p>0.11.0.0之后版本(含):</p>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./kafka-console-consumer.sh --topic __consumer_offsets --bootstrap-server localhost:9092 --formatter <span class="string">"kafka.coordinator.group.GroupMetadataManager<span class="variable">$OffsetsMessageFormatter</span>"</span> --consumer.config ../config/consumer.properties --from-beginning</span><br></pre></td></tr></tbody></table></figure>



<h3 id="消费者组案例"><a href="#消费者组案例" class="headerlink" title="消费者组案例"></a>消费者组案例</h3><p>1) 需求:测试同一个消费者组中的消费者，同一时刻只能有一个消费者消费。</p>
<p>2) 案例实操<br>        (1)在hadoop102、hadoop103 上修改/opt/module/kafka/config/consumer.properties配置<br>文件中的group.id属性为任意组名。</p>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop103 config]$ vi consumer.properties group.id=atguigu</span><br></pre></td></tr></tbody></table></figure>

<p>​        (2)在hadoop102、hadoop103 上分别启动消费者。</p>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 kafka]$ bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic first --consumer.config ../config/consumer.properties</span><br><span class="line">[atguigu@hadoop103 kafka]$ bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic first --consumer.config ../config/consumer.properties</span><br></pre></td></tr></tbody></table></figure>

<p>(3)在hadoop104上启动生产者。</p>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop104 kafka]$ bin/kafka-console-producer.sh --broker-list hadoop102:9092 --topic first</span><br></pre></td></tr></tbody></table></figure>

<h2 id="kafka高效读写数据"><a href="#kafka高效读写数据" class="headerlink" title="kafka高效读写数据"></a>kafka高效读写数据</h2><p>1)顺序写磁盘<br>    Kafka的producer生产数据，要写入到log文件中，写的过程是一直追加到文件末端,为顺序写。官网有数据表明，同样的磁盘，顺序写能到600M/s，而随机写只有100K/s。这与磁盘的机械机构有关，顺序写之所以快，是因为其省去了大量磁头寻址的时间。</p>
<p>2)零复制技术 不经过user space</p>
<p><img alt="image-20200724144505379" data-src="http://yhaq.top/group1/M00/00/02/rBGWVV8eW16AJUr6AAEOdAMibPM351.png" class="lazyload"></p>
<h2 id="Zookeeper在kafka中的作用"><a href="#Zookeeper在kafka中的作用" class="headerlink" title="Zookeeper在kafka中的作用"></a>Zookeeper在kafka中的作用</h2><p>​    Kafka集群中有一个broker会被选举为Controller，负责管理集群broker的上下线，所有topic的分区副本分配和leader选举等工作。</p>
<p>​    Controller的管理工作都是依赖于Zookeeper的</p>
<p>​    以下为partition的leader选举过程:</p>
<p><img alt="image-20200724145125510" data-src="http://yhaq.top/group1/M00/00/02/rBGWVV8eW16AP_I8AAKNrXCcwz4476.png" class="lazyload"></p>
<h2 id="kafka事务"><a href="#kafka事务" class="headerlink" title="kafka事务"></a>kafka事务</h2><h3 id="Producer事务"><a href="#Producer事务" class="headerlink" title="Producer事务"></a>Producer事务</h3><p>​    为了实现跨分区跨会话的事务，需要引入一个全局唯一的Transaction ID,并将Producer获得的PID和TransactionID绑定。这样当Producer重启后就可以通过正在进行的TransactionID获得原来的PID。</p>
<p>​    为了管理Transaction, Kafka 引入了一个新的组件Transaction Coordinator。 Producer 就是通过和Transaction Coordinator交互获得Transaction ID对应的任务状态。Transaction Coordinator还负责将事务所有写入Kafka的一个内部Topic，这样即使整个服务重启，由于事务状态得到保存，进行中的事务状态可以得到恢复，从而继续进行。</p>
<h3 id="Consumer事务"><a href="#Consumer事务" class="headerlink" title="Consumer事务"></a>Consumer事务</h3><p>​    上述事务机制主要是从Producer方面考虑，对于Consumer而言，事务的保证就会相对较弱，尤其时无法保证Commit的信息被精确消费。这是由于Consumer可以通过offset访问任意信息，而且不同的SegmentFile生命周期不同，同一事务的消息可能会出现重启后被删除的情况。</p>
<h1 id="kafka-API"><a href="#kafka-API" class="headerlink" title="kafka API"></a>kafka API</h1><h2 id="Producer-API"><a href="#Producer-API" class="headerlink" title="Producer API"></a>Producer API</h2><h3 id="消息发送流程"><a href="#消息发送流程" class="headerlink" title="消息发送流程"></a>消息发送流程</h3><p>​    Kafka的Producer发送消息采用的是<strong>异步发送</strong>的方式。在消息发送的过程中，涉及到了<strong>两个线程main线程和Sender 线程</strong>，以及<strong>一个线程共享变量RecordAccumulator</strong>。main线程将消息发送给RecordAccumulator，Sender 线程不断从RecordAccumulator中拉取消息发送到Kafka broker。</p>
<p><img alt="image-20200724164250256" data-src="http://yhaq.top/group1/M00/00/02/rBGWVV8eW16AEvtJAAK-5KDQLVI185.png" class="lazyload"></p>
<h3 id="异步发送API"><a href="#异步发送API" class="headerlink" title="异步发送API"></a>异步发送API</h3><figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyProducer</span> </span>{</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String args[])</span></span>{</span><br><span class="line">        <span class="comment">// 1.创建kafka生产者信息</span></span><br><span class="line">        Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 也可以使用</span></span><br><span class="line">        String bootstrapServersConfig = ProducerConfig.BOOTSTRAP_SERVERS_CONFIG;</span><br><span class="line">        <span class="comment">// 2.指定kafka集群</span></span><br><span class="line">        properties.put(<span class="string">"bootstrap.servers"</span>,<span class="string">"localhost:9092"</span>);</span><br><span class="line">        <span class="comment">// 3.ack应答级别</span></span><br><span class="line">        properties.put(<span class="string">"acks"</span>,<span class="string">"all"</span>);</span><br><span class="line">        <span class="comment">// 4.重试次数</span></span><br><span class="line">        properties.put(<span class="string">"retries"</span>,<span class="string">"3"</span>);</span><br><span class="line">        <span class="comment">// 5.批次大小</span></span><br><span class="line">        properties.put(<span class="string">"batch.size"</span>,<span class="number">16384</span>);</span><br><span class="line">        <span class="comment">// 6.等待时间</span></span><br><span class="line">        properties.put(<span class="string">"linger.ms"</span>,<span class="number">1</span>);</span><br><span class="line">        <span class="comment">// 7.RecordAccumulator缓存区大小</span></span><br><span class="line">        properties.put(<span class="string">"buffer.memory"</span>,<span class="number">335544</span>);</span><br><span class="line">        <span class="comment">// 8.Key，Value序列化类</span></span><br><span class="line">        properties.put(<span class="string">"key.serializer"</span>,</span><br><span class="line">                <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">        properties.put(<span class="string">"value.serializer"</span>,</span><br><span class="line">                <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//9.创建生产者对象</span></span><br><span class="line">        KafkaProducer<String, String> producer = <span class="keyword">new</span> KafkaProducer<>(properties);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 10.发送数据</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i < <span class="number">10</span>; i++) {</span><br><span class="line">            producer.send(<span class="keyword">new</span> ProducerRecord<>(<span class="string">"first"</span>,<span class="string">"kafka-message"</span>+i));</span><br><span class="line">        }</span><br><span class="line">        <span class="comment">// 11.关闭资源</span></span><br><span class="line">        producer.close();</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>

<p>带回调的生产者</p>
<figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CallBackProducer</span> </span>{</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String args[])</span></span>{</span><br><span class="line">        Properties pro = <span class="keyword">new</span> Properties();</span><br><span class="line">        pro.put(<span class="string">"bootstrap.servers"</span>,<span class="string">"localhost:9092"</span>);</span><br><span class="line">        pro.put(<span class="string">"acks"</span>,<span class="string">"all"</span>);</span><br><span class="line">        pro.put(<span class="string">"retries"</span>,<span class="string">"3"</span>);</span><br><span class="line">        pro.put(<span class="string">"batch.size"</span>,<span class="number">16384</span>);</span><br><span class="line">        pro.put(<span class="string">"linger.ms"</span>,<span class="number">1</span>);</span><br><span class="line">        pro.put(<span class="string">"buffer.memory"</span>,<span class="number">335544</span>);</span><br><span class="line">        pro.put(<span class="string">"key.serializer"</span>,</span><br><span class="line">                <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">        pro.put(<span class="string">"value.serializer"</span>,</span><br><span class="line">                <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">        KafkaProducer<String,String> producer = <span class="keyword">new</span> KafkaProducer<String, String>(pro);</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i < <span class="number">10</span>; i++) {</span><br><span class="line">            producer.send(<span class="keyword">new</span> ProducerRecord<>(<span class="string">"first"</span>, i%<span class="number">3</span>,<span class="string">""</span>,<span class="string">"test"</span> + i),</span><br><span class="line">                    (metadata, exception) -> {</span><br><span class="line">                <span class="keyword">if</span> (exception==<span class="keyword">null</span>){</span><br><span class="line">                    System.out.println(metadata.partition() + <span class="string">"--"</span> + metadata.offset());</span><br><span class="line">                }<span class="keyword">else</span>{</span><br><span class="line">                    exception.printStackTrace();</span><br><span class="line">                }</span><br><span class="line">            });</span><br><span class="line">        }</span><br><span class="line"></span><br><span class="line">        producer.close();</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>

<p>使用自己的partitioner</p>
<p>首先写一个Mypartitioner类</p>
<figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyPartitioner</span> <span class="keyword">implements</span> <span class="title">Partitioner</span> </span>{</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">partition</span><span class="params">(String topic, Object key, <span class="keyword">byte</span>[] keyBytes, Object value, <span class="keyword">byte</span>[] valueBytes, Cluster cluster)</span> </span>{</span><br><span class="line">        <span class="comment">//直接选择0号分区</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    }</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>{</span><br><span class="line">    }</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Map<String, ?> configs)</span> </span>{</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>

<p>发送者</p>
<figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">PartitionProducer</span> </span>{</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String args[])</span></span>{</span><br><span class="line">        Properties pro = <span class="keyword">new</span> Properties();</span><br><span class="line">        pro.put(<span class="string">"bootstrap.servers"</span>,<span class="string">"localhost:9092"</span>);</span><br><span class="line">        pro.put(<span class="string">"acks"</span>,<span class="string">"all"</span>);</span><br><span class="line">        pro.put(<span class="string">"retries"</span>,<span class="string">"3"</span>);</span><br><span class="line">        pro.put(<span class="string">"batch.size"</span>,<span class="number">16384</span>);</span><br><span class="line">        pro.put(<span class="string">"linger.ms"</span>,<span class="number">1</span>);</span><br><span class="line">        pro.put(<span class="string">"buffer.memory"</span>,<span class="number">335544</span>);</span><br><span class="line">        pro.put(<span class="string">"key.serializer"</span>,</span><br><span class="line">                <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">        pro.put(<span class="string">"value.serializer"</span>,</span><br><span class="line">                <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">    <span class="comment">/* 这里配置partitioner的类路径 */</span>    			pro.put(ProducerConfig.PARTITIONER_CLASS_CONFIG,<span class="string">"com.yhq.partitioner.MyPartitioner"</span>);</span><br><span class="line"></span><br><span class="line">        KafkaProducer<String,String> producer = <span class="keyword">new</span> KafkaProducer<String, String>(pro);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i < <span class="number">10</span>; i++) {</span><br><span class="line">            producer.send(<span class="keyword">new</span> ProducerRecord<>(<span class="string">"first"</span>, <span class="string">"partition"</span> + i),</span><br><span class="line">                    (metadata, exception) -> {</span><br><span class="line">                        <span class="keyword">if</span> (exception==<span class="keyword">null</span>){</span><br><span class="line">                            System.out.println(metadata.partition() + <span class="string">"--"</span> + metadata.offset());</span><br><span class="line">                        }<span class="keyword">else</span>{</span><br><span class="line">                            exception.printStackTrace();</span><br><span class="line">                        }</span><br><span class="line">                    });</span><br><span class="line">        }</span><br><span class="line"></span><br><span class="line">        producer.close();</span><br><span class="line"></span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>

<h3 id="同步发送"><a href="#同步发送" class="headerlink" title="同步发送"></a>同步发送</h3><figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//前面代码都相同</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i < <span class="number">10</span>; i++) {</span><br><span class="line">    <span class="comment">// 主要的区别是 需要等待一个数据发送之后的Future对象返回ack或者失败，然后在进行之后的操作。</span></span><br><span class="line">    <span class="comment">// 异步发送是直接发送到一个RecordAccumulator中，然后会有一个sender异步发送。</span></span><br><span class="line">    <span class="comment">// 本质上也是异步发送，但是会等待前一个发送成功之后，才会进行发送下一个数据。</span></span><br><span class="line">    Future<RecordMetadata> first = producer.send(<span class="keyword">new</span> ProducerRecord<>(<span class="string">"first"</span>, <span class="string">"partition"</span> + i));</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span> {</span><br><span class="line">        RecordMetadata recordMetadata = first.get();</span><br><span class="line"></span><br><span class="line">        System.out.println(recordMetadata.partition() + <span class="string">"---"</span> + recordMetadata.offset());</span><br><span class="line">    } <span class="keyword">catch</span> (InterruptedException e) {</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    } <span class="keyword">catch</span> (ExecutionException e) {</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>



<h2 id="Consumer-API"><a href="#Consumer-API" class="headerlink" title="Consumer API"></a>Consumer API</h2><h3 id="自动提交offest"><a href="#自动提交offest" class="headerlink" title="自动提交offest"></a>自动提交offest</h3><figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyConsumer</span> </span>{</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String args[])</span></span>{</span><br><span class="line">        <span class="comment">// 1.创建消费者配置信息</span></span><br><span class="line">        Properties pros = <span class="keyword">new</span> Properties();</span><br><span class="line">        <span class="comment">// 连接集群</span></span><br><span class="line">        pros.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG,<span class="string">"localhost:9092"</span>);</span><br><span class="line">        <span class="comment">// 开启自动提交</span></span><br><span class="line">        pros.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG,<span class="keyword">true</span>);</span><br><span class="line">        <span class="comment">// 自动提交延时</span></span><br><span class="line">        pros.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG,<span class="string">"1000"</span>);</span><br><span class="line">        <span class="comment">// key,value 的反序列化</span></span><br><span class="line">        pros.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG,</span><br><span class="line">                <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">        pros.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,</span><br><span class="line">                <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">        <span class="comment">//消费者组</span></span><br><span class="line">        pros.put(ConsumerConfig.GROUP_ID_CONFIG,<span class="string">"bigdata"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 创建消费者</span></span><br><span class="line">        KafkaConsumer<String,String> consumer = <span class="keyword">new</span> KafkaConsumer<String, String>(pros);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//订阅主题</span></span><br><span class="line">        consumer.subscribe(Collections.singletonList(<span class="string">"first"</span>));</span><br><span class="line">		<span class="comment">// 循环拉取数据</span></span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) {</span><br><span class="line">            <span class="comment">// 间隔 100ms</span></span><br><span class="line">            ConsumerRecords<String, String> poll = consumer.poll(Duration.ofMillis(<span class="number">100</span>));</span><br><span class="line">			<span class="comment">// 输出 得到的数据</span></span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord<String, String> record : poll) {</span><br><span class="line">                System.out.println(record.key() + <span class="string">"---"</span> + record.value());</span><br><span class="line">            }</span><br><span class="line"></span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>



<h3 id="手动提交offset"><a href="#手动提交offset" class="headerlink" title="手动提交offset"></a>手动提交offset</h3><p>​    虽然自动提交offset 十分简介便利，但由于其是基于时间提交的，开发人员难以把握offset提交的时机。因此Kafka还提供了手动提交offset的API。</p>
<p>​    手动提交offset的方法有两种:分别是commitSync (同步提交)和commitAsync (异步提交)。两者的相同点是，都会将本次poll的一批数据最高的偏移量提交;不同是，commititSync 阻塞当前线程，-直到提交成功，并且会自动失败重试(由不可控因素导致，也会出现提交失败) ;而commitAsync则没有失败重试机制，故有可能提交失败。</p>
<figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 开启自动提交</span></span><br><span class="line">pros.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG,<span class="keyword">false</span>);</span><br><span class="line"><span class="comment">// 自动提交延时</span></span><br><span class="line"><span class="comment">//pros.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG,"1000");</span></span><br></pre></td></tr></tbody></table></figure>

<p><strong>1）同步提交offset</strong></p>
<p>由于同步提交offset有失败重试机制，故更加可靠，以下为同步提交offset的示例：</p>
<figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CommitSyncConsumer</span> </span>{</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String args[])</span></span>{</span><br><span class="line">        Properties pros = <span class="keyword">new</span> Properties();</span><br><span class="line">        pros.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG,<span class="string">"localhost:9092"</span>);</span><br><span class="line">        pros.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG,<span class="keyword">false</span>);</span><br><span class="line"><span class="comment">//        pros.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG,"1000");</span></span><br><span class="line">        pros.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG,</span><br><span class="line">                <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">        pros.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,</span><br><span class="line">                <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">        pros.put(ConsumerConfig.GROUP_ID_CONFIG,<span class="string">"bigdata1"</span>);</span><br><span class="line"></span><br><span class="line">        KafkaConsumer<String,String> consumer = <span class="keyword">new</span> KafkaConsumer<String, String>(pros);</span><br><span class="line">        consumer.subscribe(Collections.singletonList(<span class="string">"first"</span>));</span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) {</span><br><span class="line">            ConsumerRecords<String, String> poll = consumer.poll(Duration.ofMillis(<span class="number">100</span>));</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord<String, String> record : poll) {</span><br><span class="line">                System.out.println(record.key() + <span class="string">"---"</span> + record.value());</span><br><span class="line">            }</span><br><span class="line">            <span class="comment">//同步提交，线程会阻塞到成功</span></span><br><span class="line">            consumer.commitAsync();</span><br><span class="line"></span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>

<p><strong>2）异步提交offset</strong></p>
<figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CommitAsyncConsumer</span> </span>{</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String args[])</span></span>{</span><br><span class="line">        Properties pros = <span class="keyword">new</span> Properties();</span><br><span class="line">        pros.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG,<span class="string">"localhost:9092"</span>);</span><br><span class="line">        pros.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG,<span class="keyword">false</span>);</span><br><span class="line"><span class="comment">//        pros.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG,"1000");</span></span><br><span class="line">        pros.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG,</span><br><span class="line">                <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">        pros.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,</span><br><span class="line">                <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">        pros.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG,<span class="string">"earliest"</span>);</span><br><span class="line">        pros.put(ConsumerConfig.GROUP_ID_CONFIG,<span class="string">"bigdata1"</span>);</span><br><span class="line"></span><br><span class="line">        KafkaConsumer<String,String> consumer = <span class="keyword">new</span> KafkaConsumer<String, String>(pros);</span><br><span class="line">        consumer.subscribe(Collections.singletonList(<span class="string">"first"</span>));</span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) {</span><br><span class="line">            ConsumerRecords<String, String> poll = consumer.poll(Duration.ofMillis(<span class="number">100</span>));</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord<String, String> record : poll) {</span><br><span class="line">                System.out.println(record.key() + <span class="string">"---"</span> + record.value());</span><br><span class="line">            }</span><br><span class="line">            <span class="comment">//异步提交</span></span><br><span class="line">            consumer.commitAsync(<span class="keyword">new</span> OffsetCommitCallback() {</span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onComplete</span><span class="params">(Map<TopicPartition, OffsetAndMetadata> offsets, Exception exception)</span> </span>{</span><br><span class="line">                    <span class="keyword">if</span> (exception!=<span class="keyword">null</span>){</span><br><span class="line">                        System.err.println(<span class="string">"fail commit for:"</span> +offsets);</span><br><span class="line">                    }</span><br><span class="line">                }</span><br><span class="line">            });</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>

<p><strong>3）数据漏消费和重复消费分析</strong></p>
<p>​    无论是同步提交还是异步提交offset,都有可能会造成数据的漏消费或者重复消费。先提交offset 后消费，有可能造成数据的漏消费;而先消费后提交offset, 有可能会造成数据的重复消费。</p>
<h3 id="自定义存储offset"><a href="#自定义存储offset" class="headerlink" title="自定义存储offset"></a>自定义存储offset</h3><p>​    Kafka0.9版本之前, offset存储在zookeeper, 0.9版本及之后，默认将offiet存储在Kafka的一个内置的topic中。除此之外，Kafka 还可以选择自定义存储offset。</p>
<p>​    offset的维护是相当繁琐的，因为需要考虑到消费者的Rebalace。</p>
<p>​    <strong>当有新的消费者加入消费者组、已有的消费者推出消费者组或者所订阅的主题的分区发生变化，就会触发到分区的重新分配，重新分配的过程叫做Rebalance。</strong></p>
<p>​    消费者发生Rebalance之后，每个消费者消费的分区就会发生变化。<strong>因此消费者要首先获取到自己被重新分配到的分区，并且定位到每个分区最近提交的offset位置继续消费。</strong></p>
<p>​    要实现自定义存储offset,需要借助<strong>ConsumerRebalanceListener</strong>,以下为示例代码,其中提交和获取ofset的方法，需要根据所选的offset存储系统自行实现。</p>
<figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CommitSelfConsumer</span> </span>{</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> Map<TopicPartition,Long> currentOffset = <span class="keyword">new</span> HashMap<>();</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String args[])</span></span>{</span><br><span class="line">        Properties pros = <span class="keyword">new</span> Properties();</span><br><span class="line">        pros.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG,<span class="string">"localhost:9092"</span>);</span><br><span class="line">        pros.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG,<span class="keyword">false</span>);</span><br><span class="line">        pros.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG,</span><br><span class="line">                <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">        pros.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,</span><br><span class="line">                <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">        pros.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG,<span class="string">"earliest"</span>);</span><br><span class="line">        pros.put(ConsumerConfig.GROUP_ID_CONFIG,<span class="string">"bigdata1"</span>);</span><br><span class="line"></span><br><span class="line">        KafkaConsumer<String,String> consumer = <span class="keyword">new</span> KafkaConsumer<String, String>(pros);</span><br><span class="line">        consumer.subscribe(Collections.singletonList(<span class="string">"first"</span>),</span><br><span class="line">                <span class="keyword">new</span> ConsumerRebalanceListener() {</span><br><span class="line"></span><br><span class="line">                    <span class="comment">// 在rebalance之前调用</span></span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onPartitionsRevoked</span><span class="params">(Collection<TopicPartition> partitions)</span> </span>{</span><br><span class="line">                        commitOffset(currentOffset);</span><br><span class="line">                    }</span><br><span class="line">                    <span class="comment">// 在rebalance之后调用</span></span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onPartitionsAssigned</span><span class="params">(Collection<TopicPartition> partitions)</span> </span>{</span><br><span class="line">                        currentOffset.clear();</span><br><span class="line">                        <span class="keyword">for</span> (TopicPartition partition : partitions) {</span><br><span class="line">                            <span class="comment">//定位到最新的offset进行消费</span></span><br><span class="line">                            consumer.seek(partition,getOffset(partition));</span><br><span class="line"></span><br><span class="line">                        }</span><br><span class="line">                    }</span><br><span class="line">                });</span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) {</span><br><span class="line">            ConsumerRecords<String, String> poll = consumer.poll(Duration.ofMillis(<span class="number">100</span>));</span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord<String, String> record : poll) {</span><br><span class="line">                System.out.println(record.key() + <span class="string">"---"</span> + record.value());</span><br><span class="line">                currentOffset.put(</span><br><span class="line">                        <span class="keyword">new</span> TopicPartition(record.topic(),record.partition())</span><br><span class="line">                        ,record.offset()</span><br><span class="line">                );</span><br><span class="line">                commitOffset(currentOffset);</span><br><span class="line">            }</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">    <span class="comment">// 获取某个分区的最新offset</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">long</span> <span class="title">getOffset</span><span class="params">(TopicPartition partition)</span> </span>{</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 提交offset</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">commitOffset</span><span class="params">(Map<TopicPartition,Long> currentOffset)</span></span>{</span><br><span class="line"></span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>



<h2 id="自定义拦截器"><a href="#自定义拦截器" class="headerlink" title="自定义拦截器"></a>自定义拦截器</h2><h3 id="拦截器原理"><a href="#拦截器原理" class="headerlink" title="拦截器原理"></a>拦截器原理</h3><p>​    Producer拦截器(interceptor)是在Kafka 0.10版本被引入的主要用于实现clients端的定制化控制逻辑。</p>
<p>​    对于producer而言，interceptor 使得用户在消息发送前以及producer回调逻辑前有机会对消息做一些定制化需求，比如修改消息等。同时，producer允许用户指定多个interceptor按序作用于同一条消息从而形成一个拦截链(interceptor chain)。Intercetpor 的实现接口是org.apache.kafka.clients.producer.ProduerInterceptor，其定义的方法包括: </p>
<p>​    （1）configure(configs)</p>
<p>​        获取配置信息和初始化数据时调用</p>
<p>​    （2）onSend(ProducerRecord)</p>
<p>​        该方法封装进KafkaProduer.send方法中，即它运行在用户主线程中。Producer 确保在消息被序列化以及计算分区前调用该方法。用户可以在该方法中对消息做任何操作，但最好保证不要修改消息所属的topic和分区，否则会影响目标分区的计算。</p>
<p>​    （3）onAcknowledgement(RecordMetadata， Exception)</p>
<p>​    该方法会在消息从RecordAccumulator成功发送到Kafka Broker之后，或者在发送过程中失败时调用。并且通常都是在producer 回调逻辑触发之前。onAcknowledgement 运行在producer的I0线程中，因此不要在该方法中放入很重的逻辑，否则会拖慢producer的消息发送效率。</p>
<p>​    （4）close()</p>
<p>​    关闭interceptor,主要用于执行一些资源清理工作。</p>
<p>​    如前所述，interceptor 可能被运行在多个线程中，因此在具体实现时用户需要自行确保线程安全。另外倘若指定了多个interceneptor, 则producer将按照指定顺序调用它们，并仅仅是捕获每个interceptor可能拋出的异常记录到错误日志中而非在向上传递。这在使用过程中要特别留意。</p>
<h3 id="拦截器案例"><a href="#拦截器案例" class="headerlink" title="拦截器案例"></a>拦截器案例</h3><p>1)需求:</p>
<p>​    实现一个简单的双interceptor 组成的拦截链。第一个interceptor会在消息发送前将时间戳信息加到消息value的最前部;第二个interceptor会在消息发送后更新成功发送消息数或失败发送消息数。</p>
<p><img alt="image-20200726212511912" data-src="http://yhaq.top/group1/M00/00/02/rBGWVV8eW16ALO8MAAH-nXyIlk8263.png" class="lazyload"></p>
<p>时间拦截器  在数据之前添加一个时间戳</p>
<figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TimeInterceptor</span> <span class="keyword">implements</span> <span class="title">ProducerInterceptor</span><<span class="title">String</span>,<span class="title">String</span>> </span>{</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> ProducerRecord<String, String> <span class="title">onSend</span><span class="params">(ProducerRecord<String, String> record)</span> </span>{</span><br><span class="line">        <span class="comment">// 1. 取出数据</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> ProducerRecord<>(record.topic(),</span><br><span class="line">                record.partition(),record.key(),</span><br><span class="line">                System.currentTimeMillis() + <span class="string">","</span>+record.value());</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onAcknowledgement</span><span class="params">(RecordMetadata metadata, Exception exception)</span> </span>{</span><br><span class="line"></span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>{</span><br><span class="line"></span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Map<String, ?> configs)</span> </span>{</span><br><span class="line"></span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>

<p>统计拦截器  统计成功或者失败个数    </p>
<p>需要注意只有当producer调用close方法的时候才会触发拦截器的close方法</p>
<figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CounterInterceptor</span> <span class="keyword">implements</span> <span class="title">ProducerInterceptor</span><<span class="title">String</span>,<span class="title">String</span>> </span>{</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> success;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> error;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> ProducerRecord<String, String> <span class="title">onSend</span><span class="params">(ProducerRecord<String, String> record)</span> </span>{</span><br><span class="line">        <span class="keyword">return</span> record;</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onAcknowledgement</span><span class="params">(RecordMetadata metadata, Exception exception)</span> </span>{</span><br><span class="line">        <span class="keyword">if</span> (metadata!=<span class="keyword">null</span>){</span><br><span class="line">            success++;</span><br><span class="line">        }<span class="keyword">else</span> error++;</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>{</span><br><span class="line">        System.out.println(<span class="string">"success:"</span> + success);</span><br><span class="line">        System.out.println(<span class="string">"error:"</span> + error);</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Map<String, ?> configs)</span> </span>{</span><br><span class="line"></span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>



<h1 id="kafka监控"><a href="#kafka监控" class="headerlink" title="kafka监控"></a>kafka监控</h1><h2 id="kafka-Eagle"><a href="#kafka-Eagle" class="headerlink" title="kafka Eagle"></a>kafka Eagle</h2><ol>
<li>修改kafka启动命令<br>修改kafka-server-start.sh命令中</li>
</ol>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> [ <span class="string">"x<span class="variable">$KAFKA_HEAP_OPTS</span>"</span> = <span class="string">"x"</span> ]; <span class="keyword">then</span> </span><br><span class="line">	<span class="built_in">export</span> KAFKA_HEAP_OPTS=<span class="string">"-Xmx1G -Xms1G"</span></span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></tbody></table></figure>

<p>为</p>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> [ <span class="string">"x<span class="variable">$KAFKA_HEAP_OPTS</span>"</span> = <span class="string">"x"</span>] ; <span class="keyword">then</span></span><br><span class="line">	<span class="built_in">export</span> KAFKA HEAP OPTS=<span class="string">"-server -Xms2G -Xmx2G - XX:PermSize=128m -XX:+UseG1GC -XX:MaxGCPauseMillis=200 -XX:ParallelGCThreads=8 -XX: ConcGCThreads=5 -XX:InitiatingHeapoccupancyPercent=70"</span></span><br><span class="line">	<span class="built_in">export</span> JMX_PORT=<span class="string">"9999"</span></span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></tbody></table></figure>

<p>作用是开启JMX</p>
<p>注意:修改之后在启动Kafka之前要分发之其他节点。</p>
<p>2.上传压缩包kafka-eagle-bin-1.3.7.tar.gz到集群/opt/software目录</p>
<p>3.解压到本地</p>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102software ]$ tar -zxvf kafka-eagle-bin-1.3.7.tar.gz</span><br></pre></td></tr></tbody></table></figure>

<p>4.修改启动文件执行权限</p>
<p>5.修改配置文件</p>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">kafka.eagle.zk.cluster.alias=cluster1</span><br><span class="line">cluster1.zk.list=localhost:2081</span><br><span class="line"></span><br><span class="line"><span class="comment"># offset的存储位置</span></span><br><span class="line">cluster1.kafka.eagle.offset.storage=kafka</span><br><span class="line"></span><br><span class="line">kafka.eagle.metrics.charts=<span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># jdbc driver   mysql只能使用5</span></span><br><span class="line">kafka.eagle.driver=com.mysql.jdbc.Driver</span><br><span class="line">kafka.eagle.url=jdbc:mysql://localhost:3306/kafka?useUnicode=<span class="literal">true</span>&characterEncoding=UTF-8&zeroDateTimeBehavior=convertToNull</span><br><span class="line">kafka.eagle.username=root</span><br><span class="line">kafka.eagle.password=root</span><br></pre></td></tr></tbody></table></figure>

<p>6.添加环境变量</p>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> KE_HOME=/opt/modu1e/eagle</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$KE_HOME</span>/bin</span><br></pre></td></tr></tbody></table></figure>

<p>7.启动</p>
<p>ke.sh start</p>
<blockquote>
<p>window注意%JAVA_HOME%路径不要有空格，否则会报错</p>
</blockquote>
<p><img alt="image-20200727115858135" data-src="http://yhaq.top/group1/M00/00/02/rBGWVV8eW16AP0_VAAIeTIeQm10581.png" class="lazyload"></p>
<h1 id="kafka面试题"><a href="#kafka面试题" class="headerlink" title="kafka面试题"></a>kafka面试题</h1><p>1.Kafka中的ISR(InSyncRepli)、QSR(OutSyncRepli)、 AR(AllRepli)代表什么? </p>
<blockquote>
<p>AR:所有的分区副本； ISR：所有与leader保持同步的副本集合； OSR：被leader剔除ISR的集合；当副本在一定时间未与leader进行同步就会被提出ISR。 </p>
</blockquote>
<p>2.Kafka中的HW、LEO等分别代表什么？</p>
<p>3.Kafka中是怎么体现消息顺序性的?</p>
<p>4.Kafka中的分区器、序列化器、拦截器是否了解?它们之间的处理顺序是什么?</p>
<p>5.Kafka生产者客户端的整体结构是什么样子的?使用了几个线程来处理?分别是什么?</p>
<p>6.“消费组中的消费者个数如果超过topic的分区，那么就会有消费者消费不到数据”这句话是否正确？</p>
<p>7.消费者提交消费位移时提交的是当前消费到的最新消息的offset还是offset+1?</p>
<p>8.有哪些情形会造成重复消费? </p>
<blockquote>
<p> 先消费 后提交offset，消费完成后提交offset之前系统崩溃</p>
</blockquote>
<p>9.那些情景会造成消息漏消费? </p>
<blockquote>
<p>先提交offset后消费，提交offset之后 消费之前系统崩溃</p>
</blockquote>
<p>10.当你使用kafka-topics.sh创建(删除)了一个topic之后，Kafka 背后会执行什么逻辑?<br>    1)会在zookeeper中的/brokers/topics节点下创建一个新的topic 节点，如:/brokers/ topics/first</p>
<p>​    2)触发Controller的监听程序</p>
<p>​    3) kafka Controller负责topic的创建工作，并更新metadata caches</p>
<p>11.topic的分区数可不可以增加?如果可以怎么增加?如果不可以，那又是为什么?</p>
<p>12.topic的分区数可不可以减少?如果可以怎么减少?如果不可以，那又是为什么?</p>
<blockquote>
<p>不能，会丢失数据</p>
</blockquote>
<p>13.Kafka有内部的topic吗?如果有是什么?有什么所用?</p>
<blockquote>
<p>有，记录消费者offset</p>
</blockquote>
<p>14.Kafka分区分配的概念?</p>
<p>15.简述Kafka的日志目录结构?</p>
<p>16.如果我指定了一个offset，Kafka Controller 怎么查找到对应的消息?</p>
<blockquote>
<p>1.通过文件名前缀数字x找到该绝对offset 对应消息所在文件 </p>
<p>2.offset-x为在文件中的相对偏移 </p>
<p>3.通过index文件中记录的索引找到最近的消息的位置 </p>
<p>4.从最近位置开始逐条寻找 </p>
</blockquote>
<p>17.聊一聊 Kafka Controller的作用?</p>
<blockquote>
<p> 负责kafka集群的上下线工作,所有topic的副本分区分配和选举leader工作 </p>
</blockquote>
<p>18.Kafka中有那些地方需要选举?这些地方的选举策略又有哪些? </p>
<p> <a href="https://blog.csdn.net/qq_37142346/article/details/91349100" target="_blank" rel="noopener">https://blog.csdn.net/qq_37142346/article/details/91349100</a> </p>
<p>controller leader</p>
<p>19.失效副本是指什么?有那些应对措施?</p>
<blockquote>
<p>失效副本为速率比leader相差大于10秒的follower<br>将失效的follower先剔除ISR<br>等速率接近leader10秒内,再加进ISR </p>
</blockquote>
<p>20.Kafka的哪些设计让它有如此高的性能?</p>
<p>21、Kafka中的事务是怎么实现的？</p>
<blockquote>
<p>kafka事务有两种<br>producer事务和consumer事务<br>producer事务是为了解决kafka跨分区跨会话问题<br>kafka不能跨分区跨会话的主要问题是每次启动的producer的PID都是系统随机给的<br>所以为了解决这个问题<br>我们就要手动给producer一个全局唯一的id,也就是transaction id 简称TID<br>我们将TID和PID进行绑定,在producer带着TID和PID第一次向broker注册时,broker就会记录TID,并生成一个新的组件__transaction_state用来保存TID的事务状态信息<br>当producer重启后,就会带着TID和新的PID向broker发起请求,当发现TID一致时<br>producer就会获取之前的PID,将覆盖掉新的PID,并获取上一次的事务状态信息,从而继续上次工作<br>consumer事务相对于producer事务就弱一点,需要先确保consumer的消费和提交位置为一致且具有事务功能,才能保证数据的完整,不然会造成数据的丢失或重复</p>
</blockquote>
<p>完整项目地址 <a href="https://github.com/Nigtunt/bootdemo/tree/master/kafka-demo" target="_blank" rel="noopener">https://github.com/Nigtunt/bootdemo/tree/master/kafka-demo</a> </p>
</body></html></div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Nigtunt</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://nigtunt.github.io/2020/07/27/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97kafka/">https://nigtunt.github.io/2020/07/27/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97kafka/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://nigtunt.github.io">Nigtunt</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/kafka/">kafka    </a></div><div class="post_share"><div class="social-share" data-image="https://ss0.bdstatic.com/70cFvHSh_Q1YnxGkpoWK1HF6hhy/it/u=892442603,2549350327&amp;fm=26&amp;gp=0.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script></div></div><div class="post-reward"><a class="reward-button"><i class="fa fa-qrcode"></i> 打赏<div class="reward-main"><ul class="reward-all"><li class="reward-item"><img class="lazyload post-qr-code__img" src="/img/wechat.jpg"><div class="post-qr-code__desc">微信</div></li><li class="reward-item"><img class="lazyload post-qr-code__img" src="/img/alipay.jpg"><div class="post-qr-code__desc">支付寶</div></li></ul></div></a></div><nav class="pagination_post" id="pagination"><div class="prev-post pull_left"><a href="/2020/07/28/docker-compose-and-docker-swarm/"><img class="prev_cover lazyload" data-src="https://ss1.bdstatic.com/70cFuXSh_Q1YnxGkpoWK1HF6hhy/it/u=1727875035,1058014352&amp;fm=26&amp;gp=0.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="label">上一篇</div><div class="prev_info"><span>docker compose and docker swarm</span></div></a></div><div class="next-post pull_right"><a href="/2020/06/28/springcloud%E5%85%A5%E9%97%A8%E7%BA%A7%E7%AE%80%E4%BB%8B/"><img class="next_cover lazyload" data-src="https://ss1.bdstatic.com/70cFuXSh_Q1YnxGkpoWK1HF6hhy/it/u=3448727764,759797671&amp;fm=26&amp;gp=0.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="label">下一篇</div><div class="next_info"><span>springcloud</span></div></a></div></nav></div></div><footer id="footer" style="background-image: url(https://ss0.bdstatic.com/70cFvHSh_Q1YnxGkpoWK1HF6hhy/it/u=892442603,2549350327&amp;fm=26&amp;gp=0.jpg)"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By Nigtunt</div><div class="framework-info"><span>驱动 </span><a href="http://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener"><span>Butterfly</span></a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://jerryc.me/" target="_blank" rel="noopener">blog</a>!</div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><i class="fa fa-book" id="readmode" title="阅读模式"></i><i class="fa fa-plus" id="font_plus" title="放大字体"></i><i class="fa fa-minus" id="font_minus" title="缩小字体"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="简繁转换" target="_self">繁</a><i class="darkmode fa fa-moon-o" id="darkmode" title="夜间模式"></i></div><div id="rightside-config-show"><div id="rightside_config" title="设置"><i class="fa fa-cog" aria-hidden="true"></i></div><i class="fa fa-list-ul close" id="mobile-toc-button" title="目录" aria-hidden="true"></i><i class="fa fa-arrow-up" id="go-up" title="回到顶部" aria-hidden="true"></i></div></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script async src="/js/search/local-search.js"></script><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="/js/third-party/fireworks.js"></script><script id="ribbon_piao" mobile="false" src="/js/third-party/piao.js"></script><script src="/js/baidupush.js"> </script><script src="/js/tw_cn.js"></script><script>translateInitilization()
</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@3/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@latest/lazysizes.min.js" async=""></script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>由</span> <a href="https://github.com/wzpan/hexo-generator-search" target="_blank" rel="noopener" style="color:#49B1F5;">hexo-generator-search</a>
 <span>提供支持</span></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>